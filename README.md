

# ** Data Wrangling & Vectorization**

This project explores how preprocessing and embedding choices shape text representation for NLP tasks.



## **What I Did**

* Performed vocabulary analysis on a personal document set
* Applied full preprocessing: tokenization, stopwords, lemmatization, casing, punctuation removal
* Built multiple vectorization models:

  * **TF-IDF**
  * **Word2Vec** (100/200/300 dims)
  * **Doc2Vec** (100/200/300 dims)
  * **ELMo embeddings**



## **What I Produced**

* TF-IDF summary tables + top terms
* Cosine similarity heatmaps for documents and tokens
* Comparison of embedding dimensionality
* Insights on which representations captured structure best



## **Key Insight**

Different vectorization methods reveal different types of similarity:
TF-IDF captures term importance, Word2Vec clusters semantics, Doc2Vec models document themes, and ELMo provides the strongest contextual representation.



---

If you want it **even shorter**, I can compress it to 3â€“4 lines.
